{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDataset = sklearn.datasets.load_iris()\n",
    "featuresData = irisDataset.data[:,:2]\n",
    "\n",
    "targetData = irisDataset.target\n",
    "print(featuresData)\n",
    "# print(type(featuresData))\n",
    "print(targetData)   \n",
    "featuresTrainingData , featuresTestData , targetTrainingData, targetTestData  = sklearn.model_selection.train_test_split(featuresData, targetData , test_size=0.25)\n",
    "print(\"Feature - Training Data\")\n",
    "print(featuresTrainingData)\n",
    "print(\"Feature - Test Data\")\n",
    "print(featuresTestData)\n",
    "print(\"Target - Training Data\")\n",
    "print(targetTrainingData)\n",
    "print(\"Target - Test Data\")\n",
    "print(targetTestData)\n",
    "print(len(targetTrainingData) , len(targetTestData))\n",
    "\n",
    "# let's scale the data \n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(featuresTrainingData)\n",
    "print(type(scaler))\n",
    "print('Mean',scaler.mean_)\n",
    "print(\"Variance : \",scaler.scale_)\n",
    "featuresTrainingData = scaler.transform(featuresTrainingData)\n",
    "featuresTestData = scaler.transform(featuresTestData)\n",
    "print(\"scaled Trining DATA : \")\n",
    "print(featuresTrainingData)\n",
    "print(\"scaled test DATA :\")\n",
    "print(featuresTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "# print(knn , type(knn))\n",
    "print(len(targetTrainingData))\n",
    "knn.fit(featuresTrainingData,targetTrainingData)\n",
    "result = knn.predict(featuresTestData)\n",
    "print(\"The results\")\n",
    "print(result)\n",
    "print(\"the actuals\")\n",
    "print(targetTestData)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "print(sklearn.metrics.accuracy_score(targetTestData,result))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's predict with our data\n",
    "result = knn.predict(scaler.transform([[4.3,1.2]]))\n",
    "print(result)\n",
    "print(irisDataset.target_names[result[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with 4-feature data\n",
    "irisDataset = sklearn.datasets.load_iris()\n",
    "featuresData = irisDataset.data\n",
    "\n",
    "targetData = irisDataset.target\n",
    "print(featuresData)\n",
    "# print(type(featuresData))\n",
    "print(targetData)   \n",
    "featuresTrainingData , featuresTestData , targetTrainingData, targetTestData  = sklearn.model_selection.train_test_split(featuresData, targetData , test_size=0.25)\n",
    "print(\"Feature - Training Data\")\n",
    "print(featuresTrainingData)\n",
    "print(\"Feature - Test Data\")\n",
    "print(featuresTestData)\n",
    "print(\"Target - Training Data\")\n",
    "print(targetTrainingData)\n",
    "print(\"Target - Test Data\")\n",
    "print(targetTestData)\n",
    "print(len(targetTrainingData) , len(targetTestData))\n",
    "\n",
    "# let's scale the data \n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(featuresTrainingData)\n",
    "print(type(scaler))\n",
    "print('Mean',scaler.mean_)\n",
    "print(\"Variance : \",scaler.scale_)\n",
    "featuresTrainingData = scaler.transform(featuresTrainingData)\n",
    "featuresTestData = scaler.transform(featuresTestData)\n",
    "print(\"scaled Trining DATA : \")\n",
    "print(featuresTrainingData)\n",
    "print(\"scaled test DATA :\")\n",
    "print(featuresTestData)\n",
    "\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "# print(knn , type(knn))\n",
    "print(len(targetTrainingData))\n",
    "knn.fit(featuresTrainingData,targetTrainingData)\n",
    "result = knn.predict(featuresTestData)\n",
    "print(\"The results\")\n",
    "print(result)\n",
    "print(\"the actuals\")\n",
    "print(targetTestData)\n",
    "\n",
    "# let's predict with our data\n",
    "result = knn.predict(scaler.transform([[4.3,1.2,2.3,4.2]]))\n",
    "print(result)\n",
    "print(irisDataset.target_names[result[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importent topics \n",
    "this turms or patterns followed by sociaty and this is very benifical to us.\n",
    "\n",
    "\n",
    "* classification report\n",
    "* precision / recall / F1 score / support (In context to statistics)\n",
    "* > ?Formula\n",
    "* what is gradient descent\n",
    "* Mutidimensional hyper plane\n",
    "* What happens when there are lots of features in context to \n",
    "* * **processing time / Power required**\n",
    "* Image processing / Text processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "130826a50b4030fecff09c60f733bc90a93922839765dc4abeb8c6376ff802e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
